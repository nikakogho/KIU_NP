# KIU NP Drone Show — Final Project (Nika Koghuashvili)

This repository simulates an illuminated drone show (NumPy-heavy) in **3D** with three required sub-problems:

1. **Static formation from a handwritten name image**
2. **Transition to “Happy New Year!”**
3. **Dynamic tracking of a moving object in a video with shape preservation**

**Outputs (for each sub-problem):**

* Trajectory of each drone (positions over time)
* Visualization/animation (the visualization input is the trajectory)
* Validation metrics + unit tests

All world coordinates are normalized to **[0, 100]** per axis. Final formations lie on a display plane (typically **z = 0**).

---

## Modeling choice per sub-problem (IVP)

We model the swarm as an **initial value problem (IVP)** and integrate ODEs forward in time from given initial positions/velocities until convergence.

Each drone state is defined as:

$$(x_i, v_i) \in \mathbb{R}^3 \times \mathbb{R}^3$$

We enforce the following constraints:

1.  **Velocity saturation:** $\|v_i\| \le v_{\max}$ (implemented via vector scaling, not by clamping individual components).
2.  **Collision avoidance:** A repulsive force is active within a safety radius $R_{\text{safe}}$.
3.  **Damping:** Applied to reduce oscillations and stabilize convergence.

### Core dynamics (Method 1: position target tracking + repulsion + damping)

For a drone $i$ with an assigned target $T_i(t)$, the equations of motion are:

$$
\dot{x}_i = v_i \quad (\text{with speed saturation enforced in the integrator})
$$

$$
\dot{v}_i = \frac{1}{m}\left(k_p(T_i(t)-x_i) - k_d v_i + \sum_{j\ne i} f_{\text{rep}}(x_i, x_j)\right)
$$

### Repulsion Force

The repulsive force ensures collision avoidance and is defined as:

$$
f_{\text{rep}}(x_i, x_j) = 
\begin{cases} 
0 & \text{if } \|x_i - x_j\| > R_{\text{safe}} \\
k_{\text{rep}} \frac{x_i - x_j}{\|x_i - x_j\|^3} & \text{otherwise}
\end{cases}
$$

### Numerical Method

We use the **semi-implicit (symplectic) Euler** method to preserve energy stability:

1. **Compute acceleration:** $a_t = a(x_t, v_t, T_t)$
2. **Update velocity:** $v_{t+\Delta t} = v_t + \Delta t \cdot a_t$
3. **Apply speed cap:** Ensure $\|v_{t+\Delta t}\| \le v_{\max}$
4. **Update position:** $x_{t+\Delta t} = x_t + \Delta t \cdot v_{t+\Delta t}$

This is simple and stable for second-order “spring + damping + repulsion” dynamics.

---

## Sub-problem specifics

### Sub-problem 1: Static formation (handwritten name)

**Input:** image of handwritten name (≥ 8 characters), (N), initial positions
**Task:** move from initial position to the handwritten name formation
**Output:** trajectories + visualization

Pipeline:

1. preprocess handwriting image → **binary mask**
2. mask → **N anchor pixels** (BKF anchoring)
3. anchor pixels → **world targets** in ([0,100]^3) (typically on (z=0))
4. **Hungarian assignment** (drone → target)
5. simulate Method 1 until convergence

### Sub-problem 2: Transition to “Happy New Year!”

**Input:** swarm at end of sub-problem 1, greeting text
**Task:** move to greeting formation
**Output:** trajectories + visualization

Pipeline is identical to (1), except targets are generated by **rendering text → mask → anchors → world**. The motion is smooth because the controller is continuous and damped (even though targets are fixed during this segment).

### Sub-problem 3: Dynamic tracking + shape preservation (video)

**Input:** swarm at end of sub-problem 2, a video
**Task:** dynamically repeat motion of the tracked object **while preserving formation shape**
**Output:** trajectories + visualization

We implement "shape preservation" as **rigid offsets** (translation-only). This ensures the formation maintains its structure while moving.

1. Let $x_i^{(0)}$ be the greeting formation (positions at the start of problem 3).
2. Compute the formation centroid: $c_0 = \frac{1}{N}\sum_i x_i^{(0)}$
3. Store the fixed offsets relative to the centroid: $r_i = x_i^{(0)} - c_0$

We track the object's centroid over time from the video, map it into world space as $c(t)$, and set time-varying targets:

$$
T_i(t) = c(t) + r_i
$$

So the entire "Happy New Year!" formation moves as a rigid body following the video motion.

---

## Why we do not use BVPs

We avoid boundary value problem (BVP) solvers because:

* **Inequality constraints:** Constraints like collision avoidance ($\|x_i - x_j\| \ge R_{\text{safe}}$) and speed caps ($\|v_i\| \le v_{\max}$) are awkward and computationally expensive to enforce in standard BVP formulations.
* BVPs are more brittle numerically for large coupled systems (6N states) and need careful initial guesses
* the task inputs are naturally “start here, move forward”, making IVPs simpler to implement, validate, and debug while still producing smooth trajectories

---

## Anchoring (mask → exactly N targets)

Each desired figure (handwriting, greeting text) is converted into **exactly N 3D anchor points** (one per drone).

We represent a figure as a **binary mask** (M) on a canvas:

* (M[y,x] = 1) foreground (“ink”)
* (M[y,x] = 0) background

From (M) we derive three candidate point sets:

### Candidate sets (B, K, F)

1. **Boundary / Outline (B)**
   Computed as:

* `B = M AND (NOT erosion(M))`

2. **Skeleton / Stroke centerline (K)**
   A 1-pixel-wide topology-preserving centerline, computed via iterative erosion/opening skeletonization.

3. **Fill / Interior (F)**
   Computed as:

* `F = M AND (NOT B)`

### Adaptive allocation across B/K/F

Let:

* (P = |B|) boundary pixel count
* (L = |K|) skeleton pixel count
* (N) number of drones

Density score:
$
d = \frac{N}{L + 0.35P}
$

Regimes:
* **Sparse** ($d < 0.6$):
    * `nK = N`
    * `nB = 0`
    * `nF = 0`

* **Medium** ($0.6 \le d < 1.5$):
    * `nB = floor(0.35 * N)`
    * `nK = N - nB`
    * `nF = 0`

* **Dense** ($d \ge 1.5$):
    * `nB = floor(0.25 * N)`
    * `nF = N - nB`
    * `nK = 0`

### Evenly-spaced sampling (anti-clumping)

Within each candidate set we select points using greedy **farthest-point sampling** to avoid clumps and improve readability. Selection order is legibility-first:

1. B (boundary)
2. K (skeleton)
3. F (fill)

Finally, pixel anchors ((y,x)) are mapped into world coordinates ((x,y,z)) in ([0,100]), preserving aspect ratio and centering consistently.

Example (anchoring visualization):

```bash
python -m scripts.debug_anchors --text "Happy New Year!" --N 250 --show-mask
```

![anchoring example](assets/reference_runs/anchoring_hello_world.png)

---

## Video centroid tracking (what binary opening/closing does)

For problem 3, we segment moving foreground via a simple background-difference method, then compute the centroid.

Two common cleanup operations are used (iteratively):

* **Binary opening** = erosion then dilation
  Removes small isolated noise blobs (cleans speckles).
* **Binary closing** = dilation then erosion
  Fills small holes/gaps inside the detected foreground region.

After cleanup, we take a connected component (typically the largest valid one) and compute its centroid ((y,x)) per frame, then map that centroid path into world coordinates.

---

## Validation metrics

We compute:

* minimum inter-drone distance per frame / final frame
* number of safety-violating pairs (distance < (R_{\text{safe}}))
* RMS distance to assigned targets (final)
* speed statistics (min/mean/max)

---

## Quick Start

Install dependencies:

```bash
pip install -r requirements.txt
```

Run all three sub-problems end-to-end:

```bash
python -m scripts.run_all_three \
  --video assets/arrows_moving.mp4 \
  --image assets/writing_nika_koghuashvili.png \
  --text "Happy New Year!" \
  --N 220 \
  --save_segments \
  --animate \
  --trail_seconds 1.5
```

The combined run is saved as an `.npz` (trajectories + metrics). You can also export the full animation:

* pass a path ending with `.gif` or `.mp4`:

```bash
python -m scripts.run_all_three \
  --video assets/arrows_moving.mp4 \
  --image assets/writing_nika_koghuashvili.png \
  --text "Happy New Year!" \
  --N 220 \
  --save_gif results/run_all_three.mp4
```

(Requires ffmpeg on PATH for `.mp4`.)

Example result of such run is 
[this video](assets/reference_runs/run_all_three.mp4)

---

## Tests

Run:

```bash
pytest -q
```
